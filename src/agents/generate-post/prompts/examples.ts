export const EXAMPLES = [
  `This evaluation framework just caught a body ü•ä

Just dropped a framework that measures LLM responses like we're scoring a championship fight. Custom scoring criteria, performance tracking over time, and zero mercy for hallucinations.

Check the stats: https://ai-toolkit.example.com`,

  `BREAKING: Claude 3.5 Sonnet is throwing hands with my test suite

Been in the lab all weekend with Claude 3.5. This model hits different:
- Multi-step reasoning clean like a Miami penthouse
- Handles ambiguous prompts better than my coffee handles Monday
- 73% fewer hallucinations (tested it, not just hype)
- Code gen so clean it makes my PR reviews jealous

Full breakdown (it's spicy): https://chris-vivas-blog.example.com/claude-3-5-analysis`,

  `It's May 6 2025 and rStar-Math just made small language models go super soldier serum üìà

Just watched a 3.8B parameter model solve math problems like it's been training since the 40s. Here's the secret sauce:

- Code-augmented CoT with MCTS for verified reasoning (fancy way of saying it thinks before it speaks)
- SLM-based process reward model that actually knows when math makes sense
- Self-evolution recipe that would make Dr. Erskine proud

Results hitting harder than Miami heat:
- Qwen2.5-Math-7B: 58.8% ‚Üí 90.0%
- Phi3-mini-3.8B: 41.4% ‚Üí 86.4%

This isn't just another benchmark chase - it's proof that small models can throw hands with the heavyweights when trained right.`,

  `QLoRA just dropped the memory tax by 73% üöÄ

Fine-tune a 65B parameter model on a single 48GB GPU like it's nothing. 

The democratization of AI isn't just a Miami dream anymore - it's shipping.`,

  `It's midnight and these instructions are unnatural (in a good way) üå¥

240k hard-mode prompts that'll make your model sweat. GPT-4 generated, human quality checked.

Tested across 8 benchmarks, and it's not even close - these models are learning to dance salsa with instructions.`,

  `BREAKING: VILA just entered the multimodal fight game üé®

This new vision-language model edits images like it's got an art degree from Miami Ad School. 

Natural language in, pixel-perfect edits out. No cap, just clean, honest image manipulation.`,

  `I'm watching RAG throw haymakers at hallucinations ü•ä

How it works:
1. Question drops in, gets vectorized
2. Knowledge base takes the first swing
3. Most relevant info joins your prompt in the ring
4. LLM delivers the knockout response

Result? Accuracy up, hallucinations down, and your domain knowledge stays fresher than South Beach.

Learn the combo: https://www.pinecone.io/learn/retrieval-augmented-generation/`,

  `Just caught AutoGPT working overtime in Miami üå¥

This autonomous GPT-4 agent is hustling harder than a South Beach startup:

Features hitting different:
- Internet access that never sleeps
- Memory management that doesn't quit
- Text gen that's built different
- File handling that actually makes sense

Use cases:
- Market research while you sleep
- Content that writes itself
- Data analysis on autopilot
- Code that spawns code

Catch the repo: https://github.com/Significant-Gravitas/Auto-GPT`,

  `It's May 6 2025 and GPTs just turned the App Store game upside down

Think of GPTs as your personal AI squad:
- Custom instructions that stick like Miami humidity
- Web access, code execution, DALL¬∑E vision
- API integration that actually works
- Knowledge files that hit different

Building is free, sharing is two clicks, and the possibilities? Let's just say we're just getting started.`,

  `BREAKING: Meta just dropped Llama 3 400B like it's nothing üêé

This model's got that new mixed expert architecture hitting harder than a South Beach summer:
- 128K context window (yeah, the whole document)
- Math skills that make calculators nervous
- Benchmarks between Gemini 1.5 Pro and GPT-4
- 90th-percentile grad-level math (while 70B only hits 75th)
- Sizes: 8B, 70B, 400B (choose your fighter)

Plus Meta's dropping the whole arsenal: AI Tools, Search, and Super Resolution.

Time to test this beast.`
];
